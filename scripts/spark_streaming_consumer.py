"""
Spark Streaming Consumer - ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ Kafka
ƒê√≥ng vai tr√≤ nh∆∞ Consumer trong ki·∫øn tr√∫c streaming
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, to_timestamp, current_timestamp,
    when, date_format, year, month, dayofmonth, hour, minute,
    regexp_replace, trim, lit, udf, dayofweek, length, make_date
)
import os
from pathlib import Path
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, TimestampType
)
import logging

from exchange_rate_service import ExchangeRateService

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SparkStreamingConsumer:
    def __init__(self, 
                 app_name="CreditCardStreamingConsumer",
                 kafka_bootstrap_servers="kafka-broker:9092",
                 kafka_topic="credit-card-transactions"):
        """
        Kh·ªüi t·∫°o Spark Streaming Consumer
        
        Args:
            app_name: T√™n ·ª©ng d·ª•ng Spark
            kafka_bootstrap_servers: ƒê·ªãa ch·ªâ Kafka broker
            kafka_topic: Topic ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu
        """
        self.app_name = app_name
        self.kafka_bootstrap_servers = kafka_bootstrap_servers
        self.kafka_topic = kafka_topic
        self.spark = None

        # T·∫°o base directories n·∫øu ch∆∞a t·ªìn t·∫°i
        base_path = Path("data")
        self.output_path = base_path / "output"
        self.checkpoint_path = base_path / "checkpoint"
        
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.checkpoint_path.mkdir(parents=True, exist_ok=True)
        
        # T·∫°o subdirectories
        for subdir in ["valid_transactions"]:
            (self.output_path / subdir).mkdir(parents=True, exist_ok=True)
            (self.checkpoint_path / subdir).mkdir(parents=True, exist_ok=True)


        self.exchange_service = ExchangeRateService()
        self.current_rate = self.exchange_service.get_exchange_rate()
        logger.info(f"üí± T·ªâ gi√° hi·ªán t·∫°i: {self.current_rate:,.0f} VND/USD")

        # HDFS configuration
        self.hdfs_namenode = "namenode:8020"
        self.hdfs_base_path = f"hdfs://{self.hdfs_namenode}/credit-card/processed"
        self.hdfs_checkpoint_path = f"hdfs://{self.hdfs_namenode}/credit-card/checkpoint"
    
    
    def register_exchange_rate_udf(self):
        """
        Register UDF ƒë·ªÉ convert USD sang VND
        """
        current_rate = self.current_rate
        
        @udf(returnType=DoubleType())
        def convert_usd_to_vnd(amount_usd):
            """Convert USD to VND"""
            if amount_usd is None or amount_usd <= 0:
                return None
            return float(amount_usd * current_rate)
        
        logger.info(f"‚úÖ ƒê√£ register UDF convert_usd_to_vnd (rate: {current_rate:,.0f})")
        return convert_usd_to_vnd
        
    def register_datetime_key_udf(self):
        """
        ‚úÖ Register UDF ƒë·ªÉ t·∫°o DateTime_Hour_Key
        Format: YYYY-MM-DD-HH (v√≠ d·ª•: 2024-01-15-08)
        """
        @udf(returnType=StringType())
        def create_datetime_hour_key(year, month, day, hour):
            if year is None or month is None or day is None or hour is None:
                return None
            # ‚úÖ Format: YYYY-MM-DD-HH (NOT YYYYMMDDHH)
            return f"{int(year):04d}-{int(month):02d}-{int(day):02d}-{int(hour):02d}"
        
        logger.info(f"‚úÖ ƒê√£ register UDF create_datetime_hour_key (format: YYYY-MM-DD-HH)")
        return create_datetime_hour_key
    
    def register_day_of_week_udf(self):
        """
        ‚úÖ Register UDF ƒë·ªÉ l·∫•y t√™n ng√†y trong tu·∫ßn
        """
        @udf(returnType=StringType())
        def get_day_of_week(day_of_week_num):
            if day_of_week_num is None:
                return None
            days = ["Sunday", "Monday", "Tuesday", "Wednesday", 
                   "Thursday", "Friday", "Saturday"]
            # Spark dayofweek: 1=Sunday, 2=Monday, ..., 7=Saturday
            return days[int(day_of_week_num) - 1]
        
        logger.info(f"‚úÖ ƒê√£ register UDF get_day_of_week")
        return get_day_of_week
    
    def register_is_weekend_udf(self):
        """
        ‚úÖ Register UDF ƒë·ªÉ x√°c ƒë·ªãnh weekend
        """
        @udf(returnType=StringType())
        def check_is_weekend(day_of_week_num):
            if day_of_week_num is None:
                return None
            # 1=Sunday, 7=Saturday
            return "Yes" if int(day_of_week_num) in [1, 7] else "No"
        
        logger.info(f"‚úÖ ƒê√£ register UDF check_is_weekend")
        return check_is_weekend
      
    def create_spark_session(self):
        """T·∫°o Spark Session v·ªõi c·∫•u h√¨nh Kafka"""
        try:
            self.spark = SparkSession.builder \
                .appName(self.app_name) \
                .config("spark.jars.packages", 
                        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
                .config("spark.sql.streaming.checkpointLocation", 
                        "/tmp/spark-checkpoint") \
                .config("spark.sql.shuffle.partitions", "3") \
                .config("spark.hadoop.dfs.client.use.datanode.hostname", "true") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("WARN")
            logger.info(f"‚úÖ Spark Session ƒë√£ ƒë∆∞·ª£c t·∫°o: {self.app_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ t·∫°o Spark Session: {e}")
            return False
    
    def define_schema(self):
        """
        ƒê·ªãnh nghƒ©a schema cho d·ªØ li·ªáu transaction t·ª´ CSV
        """
        return StructType([
            StructField("User", StringType(), True),
            StructField("Card", StringType(), True),
            StructField("Year", IntegerType(), True),
            StructField("Month", IntegerType(), True),
            StructField("Day", IntegerType(), True),
            StructField("Time", StringType(), True),
            StructField("Amount", StringType(), True),  # String v√¨ c√≥ d·∫•u $
            StructField("Use Chip", StringType(), True),
            StructField("Merchant Name", StringType(), True),
            StructField("Merchant City", StringType(), True),
            StructField("Merchant State", StringType(), True),
            StructField("Zip", StringType(), True),
            StructField("MCC", StringType(), True),
            StructField("Errors?", StringType(), True),
            StructField("Is Fraud?", StringType(), True),
            StructField("timestamp", StringType(), True)  # Timestamp t·ª´ producer
        ])
    
    def read_from_kafka(self):
        """
        ƒê·ªçc streaming data t·ª´ Kafka
        Spark ƒë√≥ng vai tr√≤ nh∆∞ CONSUMER ·ªü ƒë√¢y
        """
        try:
            # ƒê·ªçc stream t·ª´ Kafka
            df = self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.kafka_bootstrap_servers) \
                .option("subscribe", self.kafka_topic) \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .load()
            
            logger.info(f"‚úÖ ƒê√£ k·∫øt n·ªëi Kafka Consumer - Topic: {self.kafka_topic}")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc t·ª´ Kafka: {e}")
            return None
    
    def process_stream(self, kafka_df):
        """
        X·ª≠ l√Ω streaming data
        
        Args:
            kafka_df: DataFrame t·ª´ Kafka
        """
        # Parse JSON t·ª´ Kafka value
        schema = self.define_schema()
        
        parsed_df = kafka_df.selectExpr("CAST(value AS STRING)") \
            .select(from_json(col("value"), schema).alias("data")) \
            .select("data.*")
        
        # Register c√°c UDFs
        convert_to_vnd = self.register_exchange_rate_udf()
        create_datetime_hour_key = self.register_datetime_key_udf()
        get_day_of_week = self.register_day_of_week_udf()
        check_is_weekend = self.register_is_weekend_udf()
        
        # X·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu
        processed_df = parsed_df \
            .withColumn("Amount_USD", 
                       regexp_replace(col("Amount"), "[$,]", "").cast("double")) \
            .withColumn("Amount_VND", convert_to_vnd(col("Amount_USD"))) \
            .withColumn("Exchange_Rate", lit(int(self.current_rate))) \
            .withColumn("transaction_date", 
                       to_timestamp(col("timestamp"))) \
            .withColumn("year", year(col("transaction_date"))) \
            .withColumn("month", month(col("transaction_date"))) \
            .withColumn("day", dayofmonth(col("transaction_date"))) \
            .withColumn("hour", hour(col("transaction_date"))) \
            .withColumn("minute", minute(col("transaction_date"))) \
            .withColumn("date_str", 
                       date_format(col("transaction_date"), "dd/MM/yyyy")) \
            .withColumn("time_str", 
                       date_format(col("transaction_date"), "HH:mm:ss")) \
            .withColumn("day_of_week_num", dayofweek(col("transaction_date"))) \
            .withColumn("Day_of_Week", get_day_of_week(col("day_of_week_num"))) \
            .withColumn("Is_Weekend", check_is_weekend(col("day_of_week_num"))) \
            .withColumn("DateTime_Hour_Key", 
                       create_datetime_hour_key(col("year"), col("month"), 
                                              col("day"), col("hour"))) \
            .withColumn("Use_Chip", col("Use Chip")) \
            .withColumn("Merchant_Name", col("Merchant Name")) \
            .withColumn("Merchant_City", col("Merchant City")) \
            .withColumn("Merchant_State", col("Merchant State")) \
            .withColumn("Errors", trim(col("Errors?"))) \
            .withColumn("Is_Fraud", trim(col("Is Fraud?"))) \
            .withColumn("Processed_Timestamp", 
                       date_format(current_timestamp(), "yyyy-MM-dd HH:mm:ss")) \
            .withColumn("real_date_check", make_date(col("Year"), col("Month"), col("Day"))) \
            .withColumn("is_valid_date", col("real_date_check").isNotNull())
        
        # 1. Error Transactions: C·ªôt Errors c√≥ n·ªôi dung (B·∫•t k·ªÉ Card/Year ƒë√∫ng hay sai)
        error_transactions = processed_df \
            .filter((col("Errors").isNotNull()) & (col("Errors") != ""))

        # 2. Fraud Transactions: Is Fraud = Yes (V√† kh√¥ng ph·∫£i l√† Error)
        fraud_transactions = processed_df \
            .filter((col("Errors").isNull()) | (col("Errors") == "")) \
            .filter(col("Is_Fraud") == "Yes")

        # 3. Valid Transactions: Kh√¥ng Error, Kh√¥ng Fraud, V√Ä th·ªèa m√£n ƒëi·ªÅu ki·ªán d·ªØ li·ªáu s·∫°ch
        valid_transactions = processed_df \
            .filter((col("Errors").isNull()) | (col("Errors") == "")) \
            .filter(col("Is_Fraud") == "No") \
            .filter(col("User").isNotNull()) \
            .filter(col("Card").isNotNull()) \
            .filter(length(col("Card")) >= 16) \
            .filter(col("Amount_USD").isNotNull() & (col("Amount_USD") > 0)) \
            .filter(col("is_valid_date") == True) 

        # 4. Invalid (Ph·∫ßn c√≤n l·∫°i): Nh·ªØng c√°i kh√¥ng Error, kh√¥ng Fraud, nh∆∞ng d·ªØ li·ªáu r√°c
        invalid_df = processed_df \
            .filter((col("Errors").isNull()) | (col("Errors") == "")) \
            .filter(col("Is_Fraud") == "No") \
            .filter((col("Amount_USD").isNull()) 
                    | (col("Amount_USD") <= 0) 
                    | (length(col("Card")) < 16)
                    | (col("is_valid_date") == False)
                    ) \
            .withColumn("invalid_reason", 
                when(col("is_valid_date") == False, lit("Invalid Date"))
                .otherwise(lit("Data format invalid or missing")))

        return valid_transactions, fraud_transactions, error_transactions, invalid_df
    
    def write_to_console(self, df, output_mode="append", format_type="complete"):
        """
        Ghi d·ªØ li·ªáu ra console ƒë·ªÉ debug
        
        Args:
            df: DataFrame ƒë·ªÉ ghi
            output_mode: Mode ghi (append, complete, update)
            format_type: Lo·∫°i format (complete, compact)
        """
        truncate_value = False if format_type == "complete" else True
        
        query = df \
            .writeStream \
            .outputMode(output_mode) \
            .format("console") \
            .option("truncate", truncate_value) \
            .trigger(processingTime='5 seconds') \
            .start()
        
        return query
    
    def write_to_hdfs(self, df, hdfs_path, checkpoint_path, coalesce_partitions=1):
        """
        Ghi d·ªØ li·ªáu v√†o HDFS ·ªü ƒë·ªãnh d·∫°ng Parquet
        
        Args:
            df: DataFrame ƒë·ªÉ ghi
            hdfs_path: ƒê∆∞·ªùng d·∫´n HDFS
            checkpoint_path: ƒê∆∞·ªùng d·∫´n checkpoint
        """
        try:
            query = df \
                .coalesce(coalesce_partitions) \
                .writeStream \
                .outputMode("append") \
                .format("parquet") \
                .option("path", hdfs_path) \
                .option("checkpointLocation", checkpoint_path) \
                .partitionBy("year", "month", "day") \
                .trigger(processingTime='5 seconds') \
                .start()
            
            logger.info(f"‚úÖ ƒêang ghi d·ªØ li·ªáu v√†o HDFS: {hdfs_path}")
            return query
            
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ ghi v√†o HDFS: {e}")
            return None
    
    def write_to_csv(self, df, output_path, checkpoint_path, coalesce_partitions=1):
        """
        Ghi d·ªØ li·ªáu ra CSV file
        
        Args:
            df: DataFrame ƒë·ªÉ ghi
            output_path: ƒê∆∞·ªùng d·∫´n output
            checkpoint_path: ƒê∆∞·ªùng d·∫´n checkpoint
            coalesce_partitions: S·ªë partitions sau khi coalesce (default: 1)
        """
        try:
            # ‚úÖ Convert Path to string n·∫øu c·∫ßn
            output_path_str = str(output_path).replace("\\", "/")
            checkpoint_path_str = str(checkpoint_path).replace("\\", "/")
            
            query = df \
                .coalesce(coalesce_partitions) \
                .writeStream \
                .outputMode("append") \
                .format("csv") \
                .option("path", output_path_str) \
                .option("checkpointLocation", checkpoint_path_str) \
                .option("header", "true") \
                .trigger(processingTime='5 seconds') \
                .start()
            
            logger.info(f"‚úÖ ƒêang ghi d·ªØ li·ªáu v√†o CSV: {output_path_str}")
            return query
            
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ ghi v√†o CSV: {e}")
            return None
    
    def write_validation_logs(self, invalid_df, output_path, checkpoint_path):
        """
        Ghi log c√°c records b·ªã drop
        """
        try:
            # ‚úÖ Convert Path to string n·∫øu c·∫ßn
            output_path_str = str(output_path).replace("\\", "/")
            checkpoint_path_str = str(checkpoint_path).replace("\\", "/")
            
            query = invalid_df \
                .select("Card", "User", "Amount_USD", "invalid_reason", "timestamp") \
                .writeStream \
                .outputMode("append") \
                .format("csv") \
                .option("path", output_path_str) \
                .option("checkpointLocation", checkpoint_path_str) \
                .option("header", "true") \
                .trigger(processingTime='5 seconds') \
                .start()
            
            logger.info(f"‚úÖ ƒêang ghi validation logs v√†o: {output_path_str}")
            return query
            
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ ghi validation logs: {e}")
            return None
    
    def start_streaming(self, output_type="console"):
        """
        B·∫Øt ƒë·∫ßu streaming application
        
        Args:
            output_type: Lo·∫°i output (console, hdfs, csv, all)
        """
        if not self.create_spark_session():
            return
        
        # ƒê·ªçc t·ª´ Kafka (CONSUMER)
        logger.info("üîç ƒêang ƒë·ªçc data t·ª´ Kafka nh∆∞ m·ªôt Consumer...")
        kafka_df = self.read_from_kafka()
        if kafka_df is None:
            return
        
        # X·ª≠ l√Ω stream
        logger.info("‚öôÔ∏è  ƒêang x·ª≠ l√Ω streaming data...")
        valid_df, fraud_df, error_df, invalid_df = self.process_stream(kafka_df)
        
        # Ch·ªçn c√°c c·ªôt ƒë·ªÉ output
        output_columns = [
            "DateTime_Hour_Key",
            "User",
            "Card",
            "Year",
            "Month",
            "Day",
            "Hour",
            "Day_of_Week",
            "Is_Weekend",
            "Amount_USD",
            "Amount_VND",
            "Exchange_Rate",
            "Use_Chip",
            "Merchant_Name",
            "Merchant_City",
            "Merchant_State",
            "Zip",
            "MCC",
            "Errors",
            "Is_Fraud",
            "Processed_Timestamp"
        ]
        
        valid_output = valid_df.select(output_columns)
        
        queries = []
        
        # Console output
        if output_type in ["console", "all"]:
            logger.info("üì∫ Kh·ªüi ƒë·ªông Console Output...")
            query1 = self.write_to_console(
                valid_output.select(
                    "DateTime_Hour_Key",
                    "Card", 
                    "Merchant_Name", 
                    "Amount_USD", 
                    "Amount_VND",      
                    "Exchange_Rate",   
                    "Day_of_Week", 
                    "Is_Weekend"
                ),
                output_mode="append",
                format_type="compact"
            )
            queries.append(query1)
        
        # CSV output
        if output_type in ["csv", "all"]:
            logger.info("üìù Kh·ªüi ƒë·ªông Valid Transactions CSV Output...")
            query_valid = self.write_to_csv(
                valid_output,
                self.output_path / "valid_transactions",  # ‚úÖ S·ª≠ d·ª•ng Path object
                self.checkpoint_path / "valid_transactions",
                coalesce_partitions=1
            )
            if query_valid:
                queries.append(query_valid)
            
        # HDFS output
        if output_type in ["hdfs", "all"]:
            logger.info("üóÑÔ∏è  Kh·ªüi ƒë·ªông HDFS Output...")
            query_hdfs = self.write_to_hdfs(
                valid_output,
                f"{self.hdfs_base_path}/valid",
                f"{self.hdfs_checkpoint_path}/valid",
                coalesce_partitions=1
            )
            if query_hdfs:
                queries.append(query_hdfs)
        
        # Ch·ªù t·∫•t c·∫£ queries
        try:
            logger.info("üöÄ Spark Streaming Consumer ƒëang ch·∫°y...")
            logger.info("   Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng")
            logger.info("-" * 80)
            
            for query in queries:
                query.awaitTermination()
                
        except KeyboardInterrupt:
            logger.info("\n‚èπÔ∏è  ƒêang d·ª´ng Spark Streaming...")
            for query in queries:
                query.stop()
        finally:
            if self.spark:
                self.spark.stop()
            logger.info("‚úÖ Spark Streaming Consumer ƒë√£ d·ª´ng")


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Spark Streaming Consumer - ƒê·ªçc data t·ª´ Kafka'
    )
    parser.add_argument(
        '--broker',
        default='kafka-broker:9092',
        help='Kafka broker address (default: kafka-broker:9092)'
    )
    parser.add_argument(
        '--topic',
        default='credit-card-transactions',
        help='Kafka topic (default: credit-card-transactions)'
    )
    parser.add_argument(
        '--output',
        choices=['console', 'csv', 'hdfs', 'all'],
        default='console',
        help='Output type (default: console)'
    )
    
    args = parser.parse_args()
    
    # T·∫°o consumer
    consumer = SparkStreamingConsumer(
        kafka_bootstrap_servers=args.broker,
        kafka_topic=args.topic
    )
    
    # B·∫Øt ƒë·∫ßu streaming
    consumer.start_streaming(output_type=args.output)


if __name__ == "__main__":
    main()
