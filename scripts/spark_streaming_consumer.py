"""
Spark Streaming Consumer - Äá»c vÃ  xá»­ lÃ½ dá»¯ liá»‡u tá»« Kafka
ÄÃ³ng vai trÃ² nhÆ° Consumer trong kiáº¿n trÃºc streaming
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, to_timestamp, current_timestamp,
    when, date_format, year, month, dayofmonth, hour, minute,
    regexp_replace, trim, lit
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, TimestampType
)
import logging

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SparkStreamingConsumer:
    def __init__(self, 
                 app_name="CreditCardStreamingConsumer",
                 kafka_bootstrap_servers="kafka-broker:9092",
                 kafka_topic="credit-card-transactions"):
        """
        Khá»Ÿi táº¡o Spark Streaming Consumer
        
        Args:
            app_name: TÃªn á»©ng dá»¥ng Spark
            kafka_bootstrap_servers: Äá»‹a chá»‰ Kafka broker
            kafka_topic: Topic Ä‘á»ƒ Ä‘á»c dá»¯ liá»‡u
        """
        self.app_name = app_name
        self.kafka_bootstrap_servers = kafka_bootstrap_servers
        self.kafka_topic = kafka_topic
        self.spark = None
        
    def create_spark_session(self):
        """Táº¡o Spark Session vá»›i cáº¥u hÃ¬nh Kafka"""
        try:
            self.spark = SparkSession.builder \
                .appName(self.app_name) \
                .config("spark.jars.packages", 
                        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
                .config("spark.sql.streaming.checkpointLocation", 
                        "/tmp/spark-checkpoint") \
                .config("spark.sql.shuffle.partitions", "3") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("WARN")
            logger.info(f"âœ… Spark Session Ä‘Ã£ Ä‘Æ°á»£c táº¡o: {self.app_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ táº¡o Spark Session: {e}")
            return False
    
    def define_schema(self):
        """
        Äá»‹nh nghÄ©a schema cho dá»¯ liá»‡u transaction tá»« CSV
        """
        return StructType([
            StructField("User", StringType(), True),
            StructField("Card", StringType(), True),
            StructField("Year", IntegerType(), True),
            StructField("Month", IntegerType(), True),
            StructField("Day", IntegerType(), True),
            StructField("Time", StringType(), True),
            StructField("Amount", StringType(), True),  # String vÃ¬ cÃ³ dáº¥u $
            StructField("Use Chip", StringType(), True),
            StructField("Merchant Name", StringType(), True),
            StructField("Merchant City", StringType(), True),
            StructField("Merchant State", StringType(), True),
            StructField("Zip", StringType(), True),
            StructField("MCC", StringType(), True),
            StructField("Errors?", StringType(), True),
            StructField("Is Fraud?", StringType(), True),
            StructField("timestamp", StringType(), True)  # Timestamp tá»« producer
        ])
    
    def read_from_kafka(self):
        """
        Äá»c streaming data tá»« Kafka
        Spark Ä‘Ã³ng vai trÃ² nhÆ° CONSUMER á»Ÿ Ä‘Ã¢y
        """
        try:
            # Äá»c stream tá»« Kafka
            df = self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.kafka_bootstrap_servers) \
                .option("subscribe", self.kafka_topic) \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .load()
            
            logger.info(f"âœ… ÄÃ£ káº¿t ná»‘i Kafka Consumer - Topic: {self.kafka_topic}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c tá»« Kafka: {e}")
            return None
    
    def process_stream(self, kafka_df):
        """
        Xá»­ lÃ½ streaming data
        
        Args:
            kafka_df: DataFrame tá»« Kafka
        """
        # Parse JSON tá»« Kafka value
        schema = self.define_schema()
        
        parsed_df = kafka_df.selectExpr("CAST(value AS STRING)") \
            .select(from_json(col("value"), schema).alias("data")) \
            .select("data.*")
        
        # Xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
        processed_df = parsed_df \
            .withColumn("Amount_USD", 
                       regexp_replace(col("Amount"), "[$,]", "").cast("double")) \
            .withColumn("transaction_date", 
                       to_timestamp(col("timestamp"))) \
            .withColumn("year", year(col("transaction_date"))) \
            .withColumn("month", month(col("transaction_date"))) \
            .withColumn("day", dayofmonth(col("transaction_date"))) \
            .withColumn("hour", hour(col("transaction_date"))) \
            .withColumn("minute", minute(col("transaction_date"))) \
            .withColumn("date_str", 
                       date_format(col("transaction_date"), "dd/MM/yyyy")) \
            .withColumn("time_str", 
                       date_format(col("transaction_date"), "HH:mm:ss"))
        
        # Lá»c giao dá»‹ch há»£p lá»‡ (khÃ´ng cÃ³ lá»—i vÃ  khÃ´ng pháº£i fraud)
        valid_transactions = processed_df \
            .filter((col("Errors?").isNull()) | (trim(col("Errors?")) == "")) \
            .filter((col("Is Fraud?") != "Yes"))
        
        # Lá»c giao dá»‹ch fraud
        fraud_transactions = processed_df \
            .filter(col("Is Fraud?") == "Yes")
        
        return valid_transactions, fraud_transactions
    
    def write_to_console(self, df, output_mode="append", format_type="complete"):
        """
        Ghi dá»¯ liá»‡u ra console Ä‘á»ƒ debug
        
        Args:
            df: DataFrame Ä‘á»ƒ ghi
            output_mode: Mode ghi (append, complete, update)
            format_type: Loáº¡i format (complete, compact)
        """
        truncate_value = False if format_type == "complete" else True
        
        query = df \
            .writeStream \
            .outputMode(output_mode) \
            .format("console") \
            .option("truncate", truncate_value) \
            .start()
        
        return query
    
    def write_to_hdfs(self, df, hdfs_path, checkpoint_path):
        """
        Ghi dá»¯ liá»‡u vÃ o HDFS á»Ÿ Ä‘á»‹nh dáº¡ng Parquet
        
        Args:
            df: DataFrame Ä‘á»ƒ ghi
            hdfs_path: ÄÆ°á»ng dáº«n HDFS
            checkpoint_path: ÄÆ°á»ng dáº«n checkpoint
        """
        try:
            query = df \
                .writeStream \
                .outputMode("append") \
                .format("parquet") \
                .option("path", hdfs_path) \
                .option("checkpointLocation", checkpoint_path) \
                .start()
            
            logger.info(f"âœ… Äang ghi dá»¯ liá»‡u vÃ o HDFS: {hdfs_path}")
            return query
            
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ ghi vÃ o HDFS: {e}")
            return None
    
    def write_to_csv(self, df, output_path, checkpoint_path):
        """
        Ghi dá»¯ liá»‡u ra CSV file
        
        Args:
            df: DataFrame Ä‘á»ƒ ghi
            output_path: ÄÆ°á»ng dáº«n output
            checkpoint_path: ÄÆ°á»ng dáº«n checkpoint
        """
        try:
            query = df \
                .writeStream \
                .outputMode("append") \
                .format("csv") \
                .option("path", output_path) \
                .option("checkpointLocation", checkpoint_path) \
                .option("header", "true") \
                .start()
            
            logger.info(f"âœ… Äang ghi dá»¯ liá»‡u vÃ o CSV: {output_path}")
            return query
            
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ ghi vÃ o CSV: {e}")
            return None
    
    def start_streaming(self, output_type="console"):
        """
        Báº¯t Ä‘áº§u streaming application
        
        Args:
            output_type: Loáº¡i output (console, hdfs, csv, all)
        """
        if not self.create_spark_session():
            return
        
        # Äá»c tá»« Kafka (CONSUMER)
        logger.info("ğŸ” Äang Ä‘á»c data tá»« Kafka nhÆ° má»™t Consumer...")
        kafka_df = self.read_from_kafka()
        if kafka_df is None:
            return
        
        # Xá»­ lÃ½ stream
        logger.info("âš™ï¸  Äang xá»­ lÃ½ streaming data...")
        valid_df, fraud_df = self.process_stream(kafka_df)
        
        # Chá»n cÃ¡c cá»™t Ä‘á»ƒ output
        output_columns = [
            "Card", "transaction_date", "date_str", "time_str",
            "Merchant Name", "Merchant City", "Merchant State",
            "Amount_USD", "User", "Is Fraud?"
        ]
        
        valid_output = valid_df.select(output_columns)
        fraud_output = fraud_df.select(output_columns)
        
        queries = []
        
        # Console output
        if output_type in ["console", "all"]:
            logger.info("ğŸ“º Khá»Ÿi Ä‘á»™ng Console Output...")
            query1 = self.write_to_console(
                valid_output.select("Card", "Merchant Name", "Amount_USD", "date_str", "time_str"),
                output_mode="append",
                format_type="compact"
            )
            queries.append(query1)
        
        # CSV output
        if output_type in ["csv", "all"]:
            logger.info("ğŸ“ Khá»Ÿi Ä‘á»™ng CSV Output...")
            query2 = self.write_to_csv(
                valid_output,
                "data/output/valid_transactions",
                "data/checkpoint/valid"
            )
            query3 = self.write_to_csv(
                fraud_output,
                "data/output/fraud_transactions",
                "data/checkpoint/fraud"
            )
            if query2:
                queries.append(query2)
            if query3:
                queries.append(query3)
        
        # HDFS output
        if output_type in ["hdfs", "all"]:
            logger.info("ğŸ—„ï¸  Khá»Ÿi Ä‘á»™ng HDFS Output...")
            query4 = self.write_to_hdfs(
                valid_output,
                "hdfs://namenode:8020/transactions/valid",
                "hdfs://namenode:8020/checkpoint/valid"
            )
            query5 = self.write_to_hdfs(
                fraud_output,
                "hdfs://namenode:8020/transactions/fraud",
                "hdfs://namenode:8020/checkpoint/fraud"
            )
            if query4:
                queries.append(query4)
            if query5:
                queries.append(query5)
        
        # Chá» táº¥t cáº£ queries
        try:
            logger.info("ğŸš€ Spark Streaming Consumer Ä‘ang cháº¡y...")
            logger.info("   Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng")
            logger.info("-" * 80)
            
            for query in queries:
                query.awaitTermination()
                
        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸  Äang dá»«ng Spark Streaming...")
            for query in queries:
                query.stop()
        finally:
            if self.spark:
                self.spark.stop()
            logger.info("âœ… Spark Streaming Consumer Ä‘Ã£ dá»«ng")


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Spark Streaming Consumer - Äá»c data tá»« Kafka'
    )
    parser.add_argument(
        '--broker',
        default='kafka-broker:9092',
        help='Kafka broker address (default: kafka-broker:9092)'
    )
    parser.add_argument(
        '--topic',
        default='credit-card-transactions',
        help='Kafka topic (default: credit-card-transactions)'
    )
    parser.add_argument(
        '--output',
        choices=['console', 'csv', 'hdfs', 'all'],
        default='console',
        help='Output type (default: console)'
    )
    
    args = parser.parse_args()
    
    # Táº¡o consumer
    consumer = SparkStreamingConsumer(
        kafka_bootstrap_servers=args.broker,
        kafka_topic=args.topic
    )
    
    # Báº¯t Ä‘áº§u streaming
    consumer.start_streaming(output_type=args.output)


if __name__ == "__main__":
    main()
